[6.8610](https://mit-6861.github.io/) is MIT's graduate NLP class. Our final project investigated the reasoning robustness of LLMs on mathematical problem-solving tasks under various input perturbations using the [GSM8K](https://huggingface.co/datasets/openai/gsm8k) dataset. We evaluated the performance of thirteen state-of-the-art LLMs when exposed to four types of prompt perturbations: irrelevant context, pathological instructions, factually relevant but non-essential context, and a combination of the latter two. Our findings revealed that irrelevant context significantly impairs model performance, highlighting challenges in distinguishing essential from extraneous details. Surprisingly, performance degradation was not strictly related to task complexity or model size, and certain perturbations inadvertently triggered chain-of-thought reasoning. This work underscores the need for more robust LLMs to handle noisy and misleading inputs effectively.